{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VariTex Demo\n",
    "This notebook is about [Varitex: Variational Neural Face Textures](https://arxiv.org/pdf/2104.05988.pdf) ([ICCV 2021](https://iccv2021.thecvf.com/)). For more information, please visit the [project page](https://mcbuehler.github.io/VariTex/).\n",
    "\n",
    "![](https://ait.ethz.ch/people/buehler/public/varitex/teaser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "Please follow the instructions in the [README](https://github.com/mcbuehler/VariTex/blob/main/README.md) and download the following files:\n",
    "- [ ] the model [checkpoint](https://ait.ethz.ch/people/buehler/public/varitex/pretrained.zip)\n",
    "- [ ] the preprocessed [dataset](https://ait.ethz.ch/people/buehler/public/varitex/dataset_preprocessed.zip)\n",
    "- [ ] the [Basel Face Model](https://faces.dmi.unibas.ch/bfm/bfm2017.html)\n",
    "- [ ] the Basel Face Model [UV parameterization](https://github.com/unibas-gravis/parametric-face-image-generator/blob/master/data/regions/face12.json). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "from varitex.data.keys_enum import DataItemKey as DIK\n",
    "pl.seed_everything(1234)\n",
    "from mutil.object_dict import ObjectDict\n",
    "from torch.nn.functional import normalize as normalizeT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/matthias/ETH/Thesis/nFlows/nflows')\n",
    "#### Nflows\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.flows.realnvp import SimpleRealNVP\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.coupling import CouplingTransform, AffineCouplingTransform, AdditiveCouplingTransform\n",
    "from nflows.flows import realnvp\n",
    "sys.path.append('/home/matthias/ETH/Thesis/nFlows/pytorch-normalizing-flows/')\n",
    "\n",
    "#### NfLib\n",
    "# from nflib.flows import (\n",
    "#     AffineConstantFlow, ActNorm, AffineHalfFlow, \n",
    "#     SlowMAF, MAF, IAF, Invertible1x1Conv,\n",
    "#     NormalizingFlow, NormalizingFlowModel,\n",
    "# )\n",
    "# from nflib.spline_flows import NSF_AR, NSF_CL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import distributions\n",
    "from torch.distributions import MultivariateNormal, Uniform, TransformedDistribution, SigmoidTransform\n",
    "import itertools\n",
    "\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.flows.realnvp import SimpleRealNVP\n",
    "from nflows.flows.autoregressive import MaskedAutoregressiveFlow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedUMNNAutoregressiveTransform, MaskedPiecewiseLinearAutoregressiveTransform,MaskedPiecewiseQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation, RandomPermutation\n",
    "from nflows.transforms.nonlinearities import LeakyReLU\n",
    "from nflows.transforms.normalization import ActNorm, BatchNorm\n",
    "from nflows.transforms.coupling import CouplingTransform, AffineCouplingTransform, AdditiveCouplingTransform\n",
    "from nflows.transforms.standard import PointwiseAffineTransform\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BFM 2017 into GPU... (this can take a while)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "Res128_nonorm_path = '/home/matthias/ETH/Thesis/VariTexLocal/VariTex/output/varitex/varitex/Res128_GLO_nonorm/checkpoints/epoch=43-step=377079.ckpt'\n",
    "\n",
    "ckptFolder ='/home/matthias/ETH/Thesis/VariTexLocal/output/ckpts/Res128_GLO_NoNorm'\n",
    "filename = 'epoch=43-step=377079.ckpt'\n",
    "Res128_nonorm_path = os.path.join(ckptFolder, filename)\n",
    "Res128_norm1_path = '/home/matthias/ETH/Thesis/VariTexLocal/output/forDemo/1vc1xbec/checkpoints/epoch=43-step=377079.ckpt'\n",
    "nfFolderPath = '/home/matthias/ETH/Thesis/nFlows/nflows'\n",
    "nfCKPT = 'young-sun-6.ckpt'\n",
    "nfPath = os.path.join(nfFolderPath,nfCKPT)\n",
    "opt = {\n",
    "\n",
    "        #\"checkpoint\": os.path.join(os.getenv(\"CP\", \"pretrained/ep44.ckpt\")),\n",
    "        \"checkpoint\": Res128_nonorm_path,\n",
    "        \"dataroot_npy\": os.path.join(os.getenv(\"DP\", \"\"), 'FFHQ/preprocessed_dataset_new'),\n",
    "        \"path_bfm\": os.path.join(os.getenv(\"FP\", \"\"), \"basel_facemodel/model2017-1_face12_nomouth.h5\"),\n",
    "        \"path_uv\": os.path.join(os.getenv(\"FP\", \"\"), \"basel_facemodel/face12.json\"),\n",
    "        \"device\": \"cuda\"\n",
    "        }\n",
    "\n",
    "path_latent = os.path.join(os.getenv(\"DP\"), \"FFHQ/preprocessed_dataset_new/latents.npy\")\n",
    "\n",
    "from varitex.demo import Demo\n",
    "opt.update(use_NF=False)\n",
    "demo = Demo(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Latent Distributions\n",
    "The variables `latent_mean` and `latent_std` contain the mean and standard deviations of all samples from the holdout set, predicted by the encoder. You can choose the index of the distribution that you want to render, or you can sample from these distributions.\n",
    "\n",
    "\n",
    "Example indices from the holdout set used the [paper](https://arxiv.org/pdf/2104.05988.pdf):\n",
    "* [Teaser image](https://ait.ethz.ch/people/buehler/public/varitex/teaser.png) (Fig. 1): 184 (female), 33 (male).\n",
    "* Fig. 3: 8 (Identity 1), 904 (identity 2).\n",
    "* Fig. 5: Sampled from distributions 184, 148, 313, 319."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "latent_distributions = np.load(path_latent)\n",
    "latent_mean = torch.Tensor(latent_distributions[:,0])\n",
    "latent_std = torch.Tensor(latent_distributions[:,1])\n",
    "print(\"{} distributions loaded.\".format(latent_mean.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VariTex Controls\n",
    "\n",
    "![](https://ait.ethz.ch/people/buehler/public/varitex/varitex_pipeline_inference.png)\n",
    "\n",
    "The generator $G$ provides\n",
    "    a) __neural control__ over identity and appearance via a latent code `z` (consisting of two parts $z_{face}$ and $z_{additive}$) and \n",
    "    b) __graphical control__ over geometry (shape `sp`, expression `ep`, and pose `R`) via the parameters of a 3D morphable face model. For simplicity, let's define the pose as euler angles `theta` ($\\theta$).\n",
    "    \n",
    "$$G (z, sp, ep, R(\\theta))$$\n",
    "\n",
    "* `z` $\\in \\mathbb{R}^{256}$: Latent identity code.\n",
    "* `sp` $\\in \\mathbb{R}^{199}$ and `ep` $\\in \\mathbb{R}^{100}$: Shape and expression coefficients of the Basel Face Model. They define the geometry of the rendered face. Zeros yield a neutral face.\n",
    "* `theta` $\\in \\mathbb{R}^3$: Pose in euler angles (degrees). Zeros yield a frontal face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Control: Pose and Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape and Expression\n",
    "We load shape `sp` and expression `ep` coefficients from the FFHQ validation set. Choose any other index to load different shapes and expressions from the validation set. These parameters could also be sampled from the Basel Face Model.\n",
    "\n",
    "Change the indices for `sp` and `ep` to modify the facial expression, and shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcapath = '/home/matthias/ETH/Thesis/VariTexLocal/datasets/pcaLatents.npy'\n",
    "pca = np.load(pcapath).astype('float')\n",
    "pcaN = (pca[:,:128])\n",
    "print(np.linalg.norm(pcaN,axis=1))\n",
    "usedpca = pcaN/np.linalg.norm(pcaN,axis=1).reshape((-1,1))\n",
    "print(np.linalg.norm(usedpca,axis=1))\n",
    "print(usedpca.shape)\n",
    "mean = usedpca.mean(1).mean(0)\n",
    "std = usedpca.std(1).mean(0)\n",
    "normalDist = np.random.normal(mean, std, (70000,128))\n",
    "print(usedpca.std(1).mean(0)*100000.)\n",
    "print(normalDist.std(1).mean(0)*100000.)\n",
    "plt.hist([normalDist.flatten(),usedpca.flatten()],bins = 100,label=['G','x'], histtype='stepfilled')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.hist(stdNormal,bins = 100,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = torch.randn(100000).numpy().flatten()\n",
    "mu=1.\n",
    "sigma=1.0\n",
    "normal = torch.normal(mu,sigma,size = [100000]).numpy().flatten()\n",
    "plt.hist([standard, normal],bins = 100,color=['r','b'], histtype='stepfilled')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapes, expressions = demo.load_shape_expressions()\n",
    "index_id, index_sp, index_ep = 184, 184, 184\n",
    "sp = shapes[index_sp].unsqueeze(0)\n",
    "ep = expressions[index_ep].unsqueeze(0)\n",
    "theta = torch.Tensor((0,-35,0))\n",
    "z_sampled = (normalizeT(torch.randn(10, 256),dim=1)*1)\n",
    "\n",
    "\n",
    "norm = 0\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0)*norm, sp=sp, ep=ep, theta=theta) for z in z_sampled]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spherical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 1\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0)*norm, sp=sp, ep=ep, theta=theta) for z in z_sampled]\n",
    "print(torch.linalg.norm(z_sampled, dim=1))\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 2\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0)*norm, sp=sp, ep=ep, theta=theta) for z in z_sampled]\n",
    "print(torch.linalg.norm(z_sampled*norm, dim=1))\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 3\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0)*norm, sp=sp, ep=ep, theta=theta) for z in z_sampled]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 4\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0)*norm, sp=sp, ep=ep, theta=theta) for z in z_sampled]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 5\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0)*norm, sp=sp, ep=ep, theta=theta) for z in z_sampled]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 6\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0)*6, sp=sp, ep=ep, theta=theta) for z in z_sampled]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutil.np_util import interpolation\n",
    "n_samples = 10\n",
    "\n",
    "norm = 3\n",
    "zA, zB = (normalizeT(torch.randn(256),dim=0)*norm), (normalizeT(torch.randn(256),dim=0)*norm) \n",
    "z_interpolated = torch.Tensor(interpolation(n_samples, zA.numpy(), zB.numpy()))\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in z_interpolated]\n",
    "print(\"Norms: \", torch.linalg.norm(z_interpolated, dim=1))\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_out = [demo.runGlo(z=normalizeT(z, dim=0).unsqueeze(0)*norm, sp=sp, ep=ep, theta=theta) for z in z_interpolated]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero to norm n interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "norm = 11\n",
    "zA, zB = (torch.zeros(256).unsqueeze(0)), (normalizeT(torch.randn(256),dim=0).unsqueeze(0)*norm) \n",
    "z_interpolated = torch.Tensor(interpolation(n_samples, zA.numpy(), zB.numpy()))\n",
    "batches_out = [demo.runGlo(z=z, sp=sp, ep=ep, theta=theta) for z in z_interpolated]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = demo.pipeline.Z.weight\n",
    "latent = latent[:59990]\n",
    "latent_training_codes = latent[21:31]\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in latent_training_codes]\n",
    "print(\"Norms: \", torch.linalg.norm(latent_training_codes,dim=1))\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_training_codes = normalizeT(latent_training_codes, dim=1)*2.5\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in latent_training_codes]\n",
    "print(\"Norms: \", torch.linalg.norm(latent_training_codes,dim=1))\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_hist_full(i):\n",
    "    HIST_BINS = np.linspace(-1.5, 1.5, 100)\n",
    "    plt.hist(latent.detach().cpu().numpy().flatten(), HIST_BINS,alpha=1)\n",
    "    filename = os.path.join('output/hists/full','hist_full_epoch')\n",
    "    #plt.savefig(filename+str(i)+'full.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "def show_hist(i):\n",
    "    HIST_BINS = np.linspace(-1.5, 1.5, 100)\n",
    "    plt.hist(latent[i,:128].detach().cpu().numpy().flatten(), HIST_BINS,alpha=1)\n",
    "    plt.hist(latent[i,128:].detach().cpu().numpy(), HIST_BINS,alpha=0.6)\n",
    "    plt.title('Latent code: '+str(i))\n",
    "    filename = os.path.join('output/hists','hist_epoch')\n",
    "    plt.savefig(filename+str(i)+'.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.mean(latent,dim=1).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = torch.load(Res128_nonorm_path)['state_dict']['Z.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Flows (extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use NflowLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = demo.pipeline.Z.weight\n",
    "latent = latent[:59990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(transform, config):\n",
    "    ##Permutations\n",
    "    if(transform == 'randomPermutation'):\n",
    "        return RandomPermutation(features=config['num_features'])\n",
    "    elif(transform == 'reversePermutation'):\n",
    "        return ReversePermutation(features=config['num_features'])\n",
    "    ##Autoregressions\n",
    "    elif(transform == 'maskedAffineAR'):\n",
    "        return MaskedAffineAutoregressiveTransform(features=config['num_features'],\n",
    "                                                   hidden_features=config['hidden_features'])\n",
    "    elif(transform == 'maskedAffineARUMNN'):\n",
    "        return MaskedUMNNAutoregressiveTransform(features=config['num_features'],\n",
    "                                                 hidden_features=config['hidden_features'])\n",
    "    elif(transform == 'maskedAffineARLinear'):\n",
    "        return MaskedPiecewiseLinearAutoregressiveTransform(features=config['num_features'],\n",
    "                                                         hidden_features=config['hidden_features'],\n",
    "                                                        num_bins = 10)\n",
    "    elif(transform == 'maskedAffineARQuadratic'):\n",
    "        return MaskedPiecewiseQuadraticAutoregressiveTransform(features=config['num_features'],\n",
    "                                                        hidden_features=config['hidden_features'],\n",
    "                                                        num_bins = 10)\n",
    "    elif(transform == 'actNorm'):\n",
    "        return ActNorm(features=config['num_features'])\n",
    "    elif(transform == 'batchNorm'):\n",
    "        return BatchNorm(features=config['num_features'])\n",
    "    else:\n",
    "        print(\"Transform \"+ transform +\" is not supported\")\n",
    "\n",
    "\n",
    "def getFlow(config):\n",
    "    modelName = config['model']\n",
    "    base_dist = StandardNormal(shape=[config['num_features']])\n",
    "    if(not isinstance(modelName,list)):\n",
    "        if(modelName=='simpleRNVP'):\n",
    "            flow = realnvp.SimpleRealNVP(\n",
    "                    features = config['num_features'], \n",
    "                    hidden_features = config['hidden_features'], \n",
    "                    num_layers = config['num_layers'], \n",
    "                    num_blocks_per_layer = config['num_blocks_per_layer'],\n",
    "                    activation = config['activation'],\n",
    "                    use_volume_preserving = config['use_volume_preserving'],\n",
    "                    batch_norm_between_layers = config['batch_norm_between_layers'], \n",
    "                    batch_norm_within_layers = config['batch_norm_within_layers'],\n",
    "                    dropout_probability = config['dropout_probability'] )\n",
    "            return flow\n",
    "        elif(modelName == 'made'):\n",
    "            flow = MaskedAutoregressiveFlow(\n",
    "                    features = config['num_features'], \n",
    "                    hidden_features = config['hidden_features'], \n",
    "                    num_layers = config['num_layers'], \n",
    "                    num_blocks_per_layer = config['num_blocks_per_layer'])\n",
    "            return flow\n",
    "    else:\n",
    "        transforms = []\n",
    "        for _ in range(config['num_layers']):\n",
    "            for layer in modelName:\n",
    "                transforms.append(getTransform(layer,config))     \n",
    "    transform = CompositeTransform(transforms)\n",
    "    flow = Flow(transform, base_dist)\n",
    "    return flow\n",
    "\n",
    "def getAllFlows(expPath, exp_name, maxEpoch):\n",
    "    models = []\n",
    "    for i in range(maxEpoch):\n",
    "        currentPath = os.path.join(expPath, exp_name+str(i)+'.ckpt')\n",
    "        currentDicts = torch.load(currentPath)\n",
    "        config = currentDicts['config']\n",
    "        model = getFlow(config)\n",
    "        model.load_state_dict(currentDicts['model_state_dict'])\n",
    "        models.append(model)\n",
    "    return models\n",
    "        \n",
    "def getBatches(model, nImages=10):\n",
    "    model.eval()\n",
    "    shapes, expressions = demo.load_shape_expressions()\n",
    "    index_id, index_sp, index_ep = 184, 184, 184\n",
    "    index_id, index_sp, index_ep = 184, 184, 184\n",
    "    sp = shapes[index_sp].unsqueeze(0)\n",
    "    ep = expressions[index_ep].unsqueeze(0)\n",
    "    theta = torch.Tensor((0,-35,0))\n",
    "    samples = model.sample(nImages)\n",
    "    samples = samples.detach()\n",
    "    batches_out = [demo.runGlo(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in samples]\n",
    "    printGParams(samples.numpy(), \"current Model\")\n",
    "    return batches_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nfFolderPath = '/home/matthias/ETH/Thesis/nFlows/nflows/output'\n",
    "exp_name = 'blooming-universe-33'\n",
    "expPath = os.path.join(nfFolderPath,exp_name)\n",
    "models = getAllFlows(expPath, exp_name, 37)\n",
    "i=0\n",
    "batches_out = getBatches(models[i],10)\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10\n",
    "batches_out = getBatches(models[i],10)\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=20\n",
    "batches_out = getBatches(models[i],10)\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=30\n",
    "batches_out = getBatches(models[i],10)\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in models:\n",
    "    makeImages(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfFolderPath = '/home/matthias/ETH/Thesis/nFlows/nflows/output'\n",
    "exp_name = 'golden-energy-32'\n",
    "expPath = os.path.join(nfFolderPath,exp_name)\n",
    "cepoch = 5\n",
    "nfPath = os.path.join(expPath, exp_name+str(cepoch)+'.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowDicts = torch.load(nfPath)\n",
    "print(flowDicts.keys())\n",
    "config = flowDicts['config']\n",
    "print(config)\n",
    "flow = getFlow(config)\n",
    "model = flow\n",
    "model.load_state_dict(flowDicts['model_state_dict'])\n",
    "flowDicts['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGParams(l, name):\n",
    "    print('Mean '+name+':', \"{:.8E}\".format(l.mean(1).mean().astype('float')))\n",
    "    print('Std '+name+':', \"{:.8E}\".format(l.std(1).mean().astype('float')))\n",
    "    print('Norm '+name+':', \"{:.8E}\".format(np.linalg.norm(l, axis=1).mean().astype('float')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printGParams(latent.detach().cpu().numpy(),'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "fold = model.sample(10)\n",
    "fold = fold.detach().numpy()\n",
    "\n",
    "exLatent = latent[:10,:256].cpu()\n",
    "pLatent = model._transform(exLatent)\n",
    "exLatent = exLatent.detach().numpy()\n",
    "pLatent = pLatent[0].detach().numpy()\n",
    "\n",
    "plt.hist(exLatent.flatten(),color='b',bins=100, alpha=1,label='x')\n",
    "plt.hist(pLatent.flatten(),color='r',bins=100, alpha=0.7,label='x->z')\n",
    "#plt.hist(fnew,bins=100,alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "printGParams(exLatent,'x')\n",
    "printGParams(pLatent,'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "# fold = model.sample(n)\n",
    "# fold = fold.detach().numpy()\n",
    "exLatent = latent[:n,:256].cpu()\n",
    "exLatent = exLatent.detach().numpy()\n",
    "#plt.hist(exLatent.flatten(),color='b',bins=100, alpha=1,label='x')\n",
    "#plt.hist(fold.flatten(),color='r',bins=100, alpha=0.5,label='z->x')\n",
    "#plt.hist(pLatent.flatten(),color='r',bins=100, alpha=0.7,label='x->z')\n",
    "fold = np.random.normal(exLatent.mean(1).mean(0), exLatent.std(1).mean(0), size=(100,256))\n",
    "plt.hist([fold.flatten(),exLatent.flatten()], bins=100, alpha=0.7,label=['G','x'], stacked=False, histtype='stepfilled')\n",
    "#plt.hist(fnew,bins=100,alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "printGParams(exLatent,'x')\n",
    "printGParams(fold,'x sampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = model.sample(10)\n",
    "fold = fold.detach().numpy()\n",
    "\n",
    "plt.hist(fold.flatten(),color='b',bins=100, alpha=1,label='z->x')\n",
    "#plt.hist(pLatent.flatten(),color='r',bins=100, alpha=0.7,label='x->z')\n",
    "#plt.hist(fnew,bins=100,alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = model._distribution.sample(10)\n",
    "fold = fold.detach().numpy()\n",
    "\n",
    "plt.hist(fold.flatten(),color='b',bins=100, alpha=1,label='z')\n",
    "#plt.hist(pLatent.flatten(),color='r',bins=100, alpha=0.7,label='x->z')\n",
    "#plt.hist(fnew,bins=100,alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "printGParams(fold,'prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = model.sample(100)\n",
    "z = zs.detach().numpy()\n",
    "print(z.shape)\n",
    "z = z.flatten()\n",
    "\n",
    "shapes, expressions = demo.load_shape_expressions()\n",
    "index_id, index_sp, index_ep = 184, 184, 184\n",
    "sp = shapes[index_sp].unsqueeze(0)\n",
    "ep = expressions[index_ep].unsqueeze(0)\n",
    "theta = torch.Tensor((0,-35,0))\n",
    "\n",
    "\n",
    "z_sampled = (normalizeT(torch.randn(10, 256),dim=1)*1)\n",
    "samples = zs.detach()\n",
    "# normsSampled.append(torch.norm(samples,dim=1).mean().item())\n",
    "# normsSampled.append(torch.norm(samples,dim=1).mean().item())\n",
    "print('Groundtruth',torch.norm(latent[:1000].detach(),dim=1).mean())\n",
    "print('NF Latent sampled', torch.norm(samples,dim=1).mean())\n",
    "norm = 0\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in samples[:10]]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in exLatent[:5]:\n",
    "    plt.plot(i)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use NFLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normsSampled = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfFolderPath = '/home/matthias/ETH/Thesis/nFlows/nflows/output'\n",
    "exp_name = 'feasible-fire-18'\n",
    "nfPath = os.path.join(nfFolderPath,exp_name)\n",
    "flowArray = [] \n",
    "i=0\n",
    "while(True):\n",
    "    filename = os.path.join(nfPath, exp_name+str(i)+'.ckpt')\n",
    "    i+=1\n",
    "    if(os.path.isfile(filename)):\n",
    "        file = np.load(filename)\n",
    "        flowArray.append(file)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDicts(dicts):\n",
    "    modelArray = []\n",
    "    for i in dicts:\n",
    "        model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfFolderPath = '/home/matthias/ETH/Thesis/nFlows/nflows'\n",
    "nfCKPT = 'spring-plant-8.ckpt'\n",
    "nfCKPT = 'divine-universe-10.ckpt' \n",
    "filename = 'fallen-cosmos-12'\n",
    "filename = 'dainty-thunder-13'\n",
    "filename = 'denim-voice-15'\n",
    "\n",
    "nfFolderPath = '/home/matthias/ETH/Thesis/nFlows/nflows/output'\n",
    "exp_name = 'feasible-fire-18'\n",
    "exp_name = 'flowing-fire-19'\n",
    "exp_name = 'silver-totem-20'\n",
    "exp_name = 'crimson-bird-21'\n",
    "exp_name = 'solar-yogurt-28'\n",
    "nfPath = os.path.join(nfFolderPath,exp_name)\n",
    "cepoch = 1\n",
    "nfPath = os.path.join(nfPath, exp_name+str(cepoch)+'.ckpt')\n",
    "\n",
    "## load Nf\n",
    "flowDicts = torch.load(nfPath)\n",
    "print(flowDicts.keys())\n",
    "config = flowDicts['config']\n",
    "print(config.keys())\n",
    "nlayers = config['num_layers']\n",
    "hiddenFeatures = config['hidden_features']\n",
    "latentDim = 256\n",
    "prior = MultivariateNormal(torch.zeros(latentDim), torch.eye(latentDim))\n",
    "\n",
    "#prior = TransformedDistribution(Uniform(torch.zeros(latentDim), torch.ones(latentDim)), SigmoidTransform().inv) # Logistic distribution\n",
    "flows = [AffineHalfFlow(dim=latentDim, parity=i%2, nh=hiddenFeatures) for i in range(nlayers)]\n",
    "#norms = [ActNorm(dim=latentDim) for _ in flows]\n",
    "#flows = list(itertools.chain(*zip(norms, flows)))\n",
    "\n",
    "# flows = [Invertible1x1Conv(dim=latentDim) for i in range(nlayers)]\n",
    "# norms = [ActNorm(dim=latentDim) for _ in flows]\n",
    "# couplings = [AffineHalfFlow(dim=latentDim, parity=i%2, nh=hiddenFeatures) for i in range(len(flows))]\n",
    "# flows = list(itertools.chain(*zip(norms, flows, couplings))) # append a coupling layer after each 1x1\n",
    "    \n",
    "    \n",
    "model = NormalizingFlowModel(prior, flows)\n",
    "model.load_state_dict(flowDicts['model_state_dict'])\n",
    "print('Epoch: ',flowDicts['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "p = model.prior.sample([10])#.squeeze()\n",
    "print(p.shape)\n",
    "new, logdet = model.backward(p)\n",
    "fnew = new[-1].detach().numpy()\n",
    "plt.hist(p.detach().numpy().flatten(),color='r',bins=100, alpha=1,label='nflib prior')\n",
    "plt.hist(fnew.flatten(),bins=100,alpha=0.5, color='b', label = 'z->x')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "printGParams(p.detach().numpy(),'nflib prior')\n",
    "printGParams(fnew,'nflib z->x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = model.sample(10)\n",
    "z = zs[-1]\n",
    "zf = z.detach().numpy()\n",
    "z = zf.flatten()\n",
    "#plt.hist(z, color='r',alpha=1,bins=100, label='z->x')\n",
    "plt.hist([z,exLatent.flatten()], bins=100, alpha=0.7,label=['z->x','x'], stacked=False, histtype='stepfilled')\n",
    "#plt.hist([(exLatent+noise.numpy()).flatten(),exLatent.flatten()], bins=100, alpha=0.7,label=['z->x','x'], stacked=False, histtype='stepfilled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "printGParams(zf,'Sampled')\n",
    "printGParams(exLatent,'Groundtruth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "l = latent[:1000].detach().cpu().numpy().flatten()\n",
    "flow.eval()\n",
    "f = flow._transform(flow.sample(1000))[0].detach().numpy().flatten()\n",
    "plt.hist(l,color='r',bins=100)\n",
    "plt.hist(f,bins=100,alpha=0.5)\n",
    "plt.show()\n",
    "print('Mean latent:', l.mean().astype('float'))\n",
    "print('Mean flow sample:',f.mean().astype('float'))\n",
    "print('Std latent:',l.std().astype('float'))\n",
    "print('Std flow sample:',f.std().astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = model.sample(100)\n",
    "z = zs[-1]\n",
    "z = z.detach().numpy()\n",
    "print(z.shape)\n",
    "z = z.flatten()\n",
    "\n",
    "shapes, expressions = demo.load_shape_expressions()\n",
    "index_id, index_sp, index_ep = 184, 184, 184\n",
    "sp = shapes[index_sp].unsqueeze(0)\n",
    "ep = expressions[index_ep].unsqueeze(0)\n",
    "theta = torch.Tensor((0,-35,0))\n",
    "samples1 = flow.sample(10)\n",
    "samples2 = flow.sample(10)\n",
    "z_sampled = (normalizeT(torch.randn(10, 256),dim=1)*1)\n",
    "samples =samples1/6\n",
    "#samples = normalizeT(samples,dim=1)*3\n",
    "\n",
    "z_sampled = (normalizeT(torch.randn(10, 256),dim=1)*1)\n",
    "print(torch.norm(z_sampled,dim=1).mean())\n",
    "print('Generator Latent: ',torch.norm(samples,dim=1).mean().detach())\n",
    "samples = zs[-1].detach()\n",
    "normsSampled.append(torch.norm(samples,dim=1).mean().item())\n",
    "print('NF Latent sampled', torch.norm(samples,dim=1).mean())\n",
    "norm = 0\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in samples[:10]]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(normsSampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normsSampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non GLO demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_id, index_sp, index_ep = 184, 184, 184\n",
    "\n",
    "shapes, expressions = demo.load_shape_expressions()\n",
    "sp = shapes[index_sp].unsqueeze(0)\n",
    "ep = expressions[index_ep].unsqueeze(0)\n",
    "\n",
    "z = latent_mean[index_id].unsqueeze(0)\n",
    "theta = torch.Tensor((0,-35,0))\n",
    "\n",
    "batch_out = demo.run(z=z, sp=sp, ep=ep, theta=theta)\n",
    "demo.to_image(batch_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running multiple expressions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_indices = list(range(5))\n",
    "batches_out = [demo.run(z=z, sp=sp, ep=expressions[ep_index].unsqueeze(0), theta=theta) for ep_index in ep_indices]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose\n",
    "Pose can be defined as euler axes (degrees) `theta=(pitch, yaw, roll)`. \n",
    "\n",
    "* pitch > 0 $\\rightarrow$ looking up\n",
    "* yaw > 0 $\\rightarrow$ looking left\n",
    "* roll > 0 $\\rightarrow$ rotate clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.Tensor((-15, -20, 0))\n",
    "\n",
    "batch_out = demo.run(z=z, sp=sp, ep=ep, theta=theta)\n",
    "demo.to_image(batch_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_all = [torch.Tensor((0, -t, 0)) for t in \n",
    "             np.arange(0, 46, 15)]\n",
    "\n",
    "batches_out = [demo.run(z=z, sp=sp, ep=ep, theta=t) for t in theta_all]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Control: Identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "VariTex controls identity via a latent code `z`. You can generate variants of this identity by resampling the latent code. Execute the cell below multiple times to see different outcomes.\n",
    "\n",
    "You can change the variable `multiplier` to sample in more/less extreme regions of the learned distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 184\n",
    "multiplier = 1.5\n",
    "n_samples = 5\n",
    "z_list = torch.distributions.Normal(latent_mean[index].clone(), latent_std[index].clone() * multiplier).rsample((n_samples,)) \n",
    "theta = torch.Tensor((0, 0, 0))\n",
    "\n",
    "batches_out = [demo.run(z=z_l.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z_l in z_list]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "Interpolation between two latent codes smoothly transitions from one identity to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutil.np_util import interpolation\n",
    "n_samples = 5\n",
    "indexA, indexB = 313, 184\n",
    "\n",
    "zA, zB = latent_mean[indexA], latent_mean[indexB]\n",
    "z_interpolated = torch.Tensor(interpolation(n_samples, zA.numpy(), zB.numpy()))\n",
    "\n",
    "batches_out = [demo.run(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in z_interpolated]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = latent[:3]\n",
    "print(z.shape)\n",
    "z0, z1 = z[:,::2], z[:,1::2]\n",
    "print(z0.shape)\n",
    "print(z1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathNorms = os.getenv(\"OP\")\n",
    "pathNorms = os.path.join(pathNorms, 'norm_snapshots')\n",
    "latentsNF = []\n",
    "for i in range(15):\n",
    "    filename = 'norms_epoch_'+str(i)\n",
    "    file = os.path.join(pathNorms,filename)\n",
    "    latentNF = np.load(file+'.npy')\n",
    "    latentsNF.append(latentNF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    plt.hist(latentsNF[i][:59990].flatten(),bins=100, color='b')\n",
    "    saveName = os.path.join(pathNorms,'hist'+str(i)+'.png')\n",
    "    plt.ylim(0,5000)\n",
    "    plt.xlim(0.5,3.0)\n",
    "    plt.savefig(saveName)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from varitex.ppl_metric import dnnlib_utils\n",
    "\n",
    "sampling = 'end'\n",
    "p = torch.rand(20) * (1 if sampling == 'full' else 0)\n",
    "print(p.unsqueeze(1).unsqueeze(1).shape)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "print(device)\n",
    "\n",
    "vgg16_url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "with dnnlib_utils.open_url(vgg16_url, verbose=1) as f:\n",
    "    f_detector = torch.jit.load(f).eval().to(device)\n",
    "\n",
    "imgEx = batches_out[0][DIK.IMAGE_OUT].to(device)\n",
    "out = f_detector(imgEx,resize_images=False, return_lpips=True)\n",
    "print(out.shape)\n",
    "\n",
    "f_detector.eval()\n",
    "f_detector.\n",
    "\n",
    "def slerp(a, b, t):\n",
    "    a = a / a.norm(dim=-1, keepdim=True)\n",
    "    b = b / b.norm(dim=-1, keepdim=True)\n",
    "    d = (a * b).sum(dim=-1, keepdim=True)\n",
    "    p = t * torch.acos(d)\n",
    "    c = b - d * a\n",
    "    c = c / c.norm(dim=-1, keepdim=True)\n",
    "    d = a * torch.cos(p) + c * torch.sin(p)\n",
    "    d = d / d.norm(dim=-1, keepdim=True)\n",
    "    return d\n",
    "\n",
    "class PPLSampler(torch.nn.Module):\n",
    "    def __init__(self, model='encoder', epsilon= 1e-4, interpolation = 'linear', sampling, crop, vgg16):\n",
    "    def samplePPL():\n",
    "    #StyleGAN takes generator, epsilon, space, sampling = end/full crop, vgg\n",
    "    # 1. Randomly choose latents and interpolations\n",
    "    if(model=='encoder'):\n",
    "        \n",
    "    elif(model=='GLO'):\n",
    "        \n",
    "    else:\n",
    "        #model with NF\n",
    "    # 2. Interpolate\n",
    "    if(interpolation=='linear'):\n",
    "        \n",
    "    else: \n",
    "    # 3. Generate images\n",
    "    \n",
    "    # 4 Calc Lpips and distance\n",
    "    lpips_t0, lpips_t1 = self.vgg16(img, resize_images=False, return_lpips=True).chunk(2)\n",
    "    dist = (lpips_t0 - lpips_t1).square().sum(1) / self.epsilon ** 2\n",
    "    \n",
    "    \n",
    "    \n",
    "def computePPL()\n",
    "\n",
    "\n",
    "_feature_detector_cache = dict()\n",
    "def get_feature_detector(url, device=torch.device('cpu'), num_gpus=1, rank=0, verbose=False):\n",
    "    assert 0 <= rank < num_gpus\n",
    "    key = (url, device)\n",
    "    if key not in _feature_detector_cache:\n",
    "        is_leader = (rank == 0)\n",
    "        if not is_leader and num_gpus > 1:\n",
    "            torch.distributed.barrier() # leader goes first\n",
    "        with dnnlib.util.open_url(url, verbose=(verbose and is_leader)) as f:\n",
    "            _feature_detector_cache[key] = torch.jit.load(f).eval().to(device)\n",
    "        if is_leader and num_gpus > 1:\n",
    "            torch.distributed.barrier() # others follow\n",
    "    return _feature_detector_cache[key]\n",
    "\n",
    "print(torch.randn([3 * 2, 256], device=device).chunk(2)[0].shape)\n",
    "\n",
    "\n",
    "# Calculate FIDs\n",
    "\n",
    "## Standard (Training)\n",
    "\n",
    "## Interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "opt.update({\"dataset\": 'FFHQ',\n",
    "            \"dataroot_npy\": os.path.join(os.getenv(\"DP\"), 'FFHQ/preprocessed_dataset_new'),\n",
    "            \"image_folder\": os.path.join(os.getenv(\"DP\"), 'FFHQ/images'),\n",
    "            \"transform_mode\": 'all',\n",
    "            \"image_h\": 128,\n",
    "            \"image_w\": 128,\n",
    "            \"batch_size\": 7,\n",
    "            \"num_workers\": 12,\n",
    "            \"semantic_regions\": list(range(1, 16)),\n",
    "            \"keep_background\": False,\n",
    "            \"bg_color\" : 'black',\n",
    "            \"latent_dim\": 256\n",
    "                        })\n",
    "opt_new = ObjectDict(opt)\n",
    "\n",
    "from varitex.data.npy_dataset import NPYDataset\n",
    "train_dataset = NPYDataset(opt_new, split=\"train\", augmentation=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=opt_new.batch_size, num_workers=opt_new.num_workers,\n",
    "                                      shuffle=False, drop_last=True)\n",
    "\n",
    "ckptFolder ='/home/matthias/ETH/Thesis/VariTexLocal/output/ckpts/Res128_GLO_NoNorm'\n",
    "filename = 'epoch=43-step=377079.ckpt'\n",
    "Res128_nonorm_path = os.path.join(ckptFolder, filename)\n",
    "Z = np.load(Res128_nonorm_path)\n",
    "for batch in train_dataloader:\n",
    "    print(batch.keys())\n",
    "    print(batch[DIK.IMAGE_IN].shape)\n",
    "    imgT = batch[DIK.IMAGE_IN]\n",
    "    #imgT = [k for k in imgT]\n",
    "    demo.visualizer_complete.tensor2image(imgT[0].squeeze(), return_format='pil')\n",
    "    break\n",
    "demo.visualizer_complete.tensor2image(imgT[0].squeeze(), return_format='pil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = batch[DIK.COEFF_SHAPE][0].unsqueeze(0)\n",
    "ep = batch[DIK.COEFF_EXPRESSION][0].unsqueeze(0)\n",
    "# shapes, expressions = demo.load_shape_expressions()\n",
    "# index_id, index_sp, index_ep = 184, 184, 184\n",
    "# sp = shapes[index_sp].unsqueeze(0)\n",
    "# ep = expressions[index_ep].unsqueeze(0)\n",
    "theta = torch.Tensor((0,-100,0))\n",
    "Z = demo.pipeline.Z.weight\n",
    "Z = Z[:59990]\n",
    "batches_out = [demo.runGlo(z=z.unsqueeze(0), sp=sp, ep=ep, theta=theta) for z in Z[:10]]\n",
    "demo.to_image(batches_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "\n",
    "data = train_dataset.data[\"images\"][0]\n",
    "data  = np.array(data).astype('int')\n",
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=['default', 'norm', 'nonorm', 'nf_glo_alternate', 'nf_glo_joint']\n",
    "def getModel(modelName, opt):\n",
    "    mp_path = '/cluster/project/infk/hilliges/koenigma/Final_Models'\n",
    "    mp_path = '/home/matthias/ETH/Thesis/Final_Models'\n",
    "    if (modelName == 'default'):\n",
    "        folder = os.path.join(mp_path, 'default/checkpoints')\n",
    "        opt.update({\"checkpoint\": os.path.join(folder, 'epoch=43-step=377079.ckpt'),\n",
    "                    \"use_glo\": False,\n",
    "                    \"use_NF\": False,\n",
    "                    \"alternate\": False,\n",
    "                    \"experiment_name\": \"eval_Default\"})\n",
    "    elif (modelName == 'norm'):\n",
    "        folder = os.path.join(mp_path, 'GLO_Norm/checkpoints')\n",
    "        opt.update({\"checkpoint\": os.path.join(folder, 'epoch=43-step=377079.ckpt'),\n",
    "                    \"use_glo\": True,\n",
    "                    \"use_NF\": False,\n",
    "                    \"alternate\": False,\n",
    "                    \"experiment_name\": \"eval_norm\"})\n",
    "    elif (modelName == 'nonorm'):\n",
    "        folder = os.path.join(mp_path, 'GLO_NoNorm/checkpoints')\n",
    "        opt.update({\"checkpoint\": os.path.join(folder, 'epoch=43-step=377079.ckpt'),\n",
    "                    \"use_glo\": True,\n",
    "                    \"use_NF\": False,\n",
    "                    \"alternate\": False,\n",
    "                    \"experiment_name\": \"eval_nonorm\"})\n",
    "    elif (modelName == 'nf_glo_alternate'):\n",
    "        folder = os.path.join(mp_path, 'GLO_NF_ALTERNATE')\n",
    "        opt.update({\"checkpoint\": os.path.join(folder, 'epoch=35-step=308519.ckpt'),\n",
    "                    \"use_glo\": True,\n",
    "                    \"use_NF\": True,\n",
    "                    \"alternate\": True,\n",
    "                    \"experiment_name\": \"eval_nf_glo_alternate\"})\n",
    "    elif (modelName == 'nf_glo_joint'):\n",
    "        folder = os.path.join(mp_path, 'GLO_NF_joint/Res128_GLO_NF_lambdae-3')\n",
    "        opt.update({\"checkpoint\": os.path.join(folder, 'epoch=43-step=377079.ckpt'),\n",
    "                    \"use_glo\": True,\n",
    "                    \"use_NF\": True,\n",
    "                    \"alternate\": False,\n",
    "                    \"experiment_name\": \"eval_nf_glo_joint\"})\n",
    "    else:\n",
    "        print('Model not recognized!')\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets Loaded\n",
      "Loading BFM 2017 into GPU... (this can take a while)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from varitex.evaluation.inference import inference_ffhq\n",
    "from varitex import validation\n",
    "#def do_validation(model_name, opt):\n",
    "name = 'default'\n",
    "opt.update({\"dataset\": 'FFHQ',\n",
    "        \"dataroot_npy\": os.path.join(os.getenv(\"DP\"), 'FFHQ/preprocessed_dataset_new'),\n",
    "        \"image_folder\": os.path.join(os.getenv(\"DP\"), 'FFHQ/images'),\n",
    "        \"transform_mode\": 'all',\n",
    "        \"image_h\": 128,\n",
    "        \"image_w\": 128,\n",
    "        \"batch_size\": 1,\n",
    "        \"num_workers\": 12,\n",
    "        \"semantic_regions\": list(range(1, 16)),\n",
    "        \"keep_background\": False,\n",
    "        \"bg_color\" : 'black',\n",
    "        \"latent_dim\": 256,\n",
    "        \"texture_dim\" : 128,\n",
    "        \"texture_nc\": 16,\n",
    "        \"nc_feature2image\": 64,\n",
    "        \"feature2image_num_layers\": 5,\n",
    "        \"use_glo\": False,\n",
    "        \"glo_init\":'pca',\n",
    "        \"pca_file\": os.path.join(os.getenv(\"BP\"), 'datasets/pcaLatents.npy'),\n",
    "        \"experiment_name\": 'eval_NoNorm',\n",
    "        \"use_NF\": False,\n",
    "        \"eval\": True})\n",
    "opt = getModel(name,opt)\n",
    "opt_new = ObjectDict(opt)\n",
    "vals = validation.Validation(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def printJSON(jsonPath):\n",
    "    #jsonPath = os.path.join(jsonPath, 'standards.json')\n",
    "    try:\n",
    "        f = open(jsonPath,)        \n",
    "    except IOError:\n",
    "        return 'NaN'\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    print(data)\n",
    "    return data['FID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "finalPath = '/home/matthias/ETH/Thesis/Euler/Final_Models'\n",
    "paths.append(os.path.join(finalPath,'default/checkpoints'))\n",
    "paths.append(os.path.join(finalPath,'GLO_Norm/checkpoints'))\n",
    "paths.append(os.path.join(finalPath,'GLO_NoNorm/checkpoints'))\n",
    "paths.append(os.path.join(finalPath,'GLO_NF_joint/Res128_GLO_NF_lambdae-3'))\n",
    "paths.append(os.path.join(finalPath,'GLO_NF_ALTERNATE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidFiles = []\n",
    "fidFiles.append('FID_constant_interpolated_linear_sampling_latent.json')\n",
    "fidFiles.append('FID_constant_interpolated_None_sampling_latent.json')\n",
    "fidFiles.append('FID_sampled_interpolated_linear_sampling_latent.json')\n",
    "fidFiles.append('FID_sampled_interpolated_None_sampling_latent.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FID': 29.466691970825195}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/default/checkpoints with FID: FID_constant_interpolated_linear_sampling_latent.json 29.466691970825195\n",
      "{'FID': 29.182600021362305}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_Norm/checkpoints with FID: FID_constant_interpolated_linear_sampling_latent.json 29.182600021362305\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NoNorm/checkpoints with FID: FID_constant_interpolated_linear_sampling_latent.json NaN\n",
      "{'FID': 24.900615692138672}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_joint/Res128_GLO_NF_lambdae-3 with FID: FID_constant_interpolated_linear_sampling_latent.json 24.900615692138672\n",
      "{'FID': 677.7301025390625}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_ALTERNATE with FID: FID_constant_interpolated_linear_sampling_latent.json 677.7301025390625\n",
      "{'FID': 25.726268768310547}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/default/checkpoints with FID: FID_constant_interpolated_None_sampling_latent.json 25.726268768310547\n",
      "{'FID': 29.182600021362305}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_Norm/checkpoints with FID: FID_constant_interpolated_None_sampling_latent.json 29.182600021362305\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NoNorm/checkpoints with FID: FID_constant_interpolated_None_sampling_latent.json NaN\n",
      "{'FID': 24.900615692138672}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_joint/Res128_GLO_NF_lambdae-3 with FID: FID_constant_interpolated_None_sampling_latent.json 24.900615692138672\n",
      "{'FID': 677.7301025390625}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_ALTERNATE with FID: FID_constant_interpolated_None_sampling_latent.json 677.7301025390625\n",
      "{'FID': 18.81654167175293}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/default/checkpoints with FID: FID_sampled_interpolated_linear_sampling_latent.json 18.81654167175293\n",
      "{'FID': 20.505517959594727}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_Norm/checkpoints with FID: FID_sampled_interpolated_linear_sampling_latent.json 20.505517959594727\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NoNorm/checkpoints with FID: FID_sampled_interpolated_linear_sampling_latent.json NaN\n",
      "{'FID': 17.375240325927734}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_joint/Res128_GLO_NF_lambdae-3 with FID: FID_sampled_interpolated_linear_sampling_latent.json 17.375240325927734\n",
      "{'FID': 677.7301025390625}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_ALTERNATE with FID: FID_sampled_interpolated_linear_sampling_latent.json 677.7301025390625\n",
      "{'FID': 17.204620361328125}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/default/checkpoints with FID: FID_sampled_interpolated_None_sampling_latent.json 17.204620361328125\n",
      "{'FID': 20.1346435546875}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_Norm/checkpoints with FID: FID_sampled_interpolated_None_sampling_latent.json 20.1346435546875\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NoNorm/checkpoints with FID: FID_sampled_interpolated_None_sampling_latent.json NaN\n",
      "{'FID': 17.430477142333984}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_joint/Res128_GLO_NF_lambdae-3 with FID: FID_sampled_interpolated_None_sampling_latent.json 17.430477142333984\n",
      "{'FID': 677.7301025390625}\n",
      "/home/matthias/ETH/Thesis/Euler/Final_Models/GLO_NF_ALTERNATE with FID: FID_sampled_interpolated_None_sampling_latent.json 677.7301025390625\n"
     ]
    }
   ],
   "source": [
    "modelFIDS = np.zeros((4,5))\n",
    "i=0\n",
    "j=0\n",
    "for fid in fidFiles:\n",
    "    j=0\n",
    "    for modelPath in paths:\n",
    "        fidVal = printJSON(os.path.join(modelPath, fid))\n",
    "        modelFIDS[i,j] = fidVal\n",
    "        print(modelPath + ' with FID: ' + fid + ' '  + str(fidVal))\n",
    "        j+=1\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFIDS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.452"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(3.4521, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableToLatex(arr):\n",
    "    Models = np.array(['Baseline', 'GLO_Norm', 'GLO_NoNorm', 'Joint', 'Alternate'])\n",
    "    cols, rows = arr.shape\n",
    "    table = ''\n",
    "    for i in range(rows):\n",
    "        row = Models[i]\n",
    "        for j in range(cols):\n",
    "            row+=' & ' + str(np.round(arr[j,i],3))\n",
    "        print(row + ' \\\\\\\\')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline & 29.467 & 25.726 & 18.817 & 17.205 \\\\\n",
      "GLO_Norm & 29.183 & 29.183 & 20.506 & 20.135 \\\\\n",
      "GLO_NoNorm & nan & nan & nan & nan \\\\\n",
      "Joint & 24.901 & 24.901 & 17.375 & 17.43 \\\\\n",
      "Alternate & 677.73 & 677.73 & 677.73 & 677.73 \\\\\n"
     ]
    }
   ],
   "source": [
    "tableToLatex(modelFIDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29.46669197,  29.18260002,          nan,  24.90061569,\n",
       "        677.73010254],\n",
       "       [ 25.72626877,  29.18260002,          nan,  24.90061569,\n",
       "        677.73010254],\n",
       "       [ 18.81654167,  20.50551796,          nan,  17.37524033,\n",
       "        677.73010254],\n",
       "       [ 17.20462036,  20.13464355,          nan,  17.43047714,\n",
       "        677.73010254]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matthias/ETH/Thesis/Euler/Final_Models/default/checkpoints'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 'something'\n",
    "if(not(shape ==  None)):\n",
    "    print('True')\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vals.opt.experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = vals.sample()\n",
    "vals.inference_ffhq(opt_new, n=10000)\n",
    "# psnr = np.array(vals.psnr).mean()\n",
    "# ssim = np.array(vals.ssim).mean()\n",
    "# lpips = np.array(vals.lpips).mean()\n",
    "fid_std = vals.fid_std\n",
    "print(fid_std)\n",
    "# print('PSNR: ' + str(psnr), 'SSIM: '+ str(ssim), 'LPIPS: ' + str(lpips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, metric, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psnr, ssim, lpips, vals.fid_std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid  = vals.metric_fid_std.fid.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = do_validation('norm', opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for i, batch in enumerate(vals.dataloader):\n",
    "    real = batch[DIK.IMAGE_IN]\n",
    "    demo.visualizer_complete.tensor2image(real[0].squeeze(), return_format='pil')\n",
    "    if(idx==i):\n",
    "        break\n",
    "\n",
    "demo.visualizer_complete.tensor2image(real[1].squeeze(), return_format='pil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.visualizer_complete.tensor2image(real[0].squeeze(), return_format='pil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = torch.rand(1).to(vals.device)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(1)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slerp(a, b, t):\n",
    "    a = a / a.norm(dim=-1, keepdim=True)\n",
    "    b = b / b.norm(dim=-1, keepdim=True)\n",
    "    d = (a * b).sum(dim=-1, keepdim=True)\n",
    "    p = t * torch.acos(d)\n",
    "    c = b - d * a\n",
    "    c = c / c.norm(dim=-1, keepdim=True)\n",
    "    d = a * torch.cos(p) + c * torch.sin(p)\n",
    "    d = d / d.norm(dim=-1, keepdim=True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.visualizer_complete.tensor2image(slerp(real[0],real[1],0.01).squeeze(), return_format='pil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.visualizer_complete.tensor2image(i[idx].squeeze(), return_format='pil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.visualizer_complete.tensor2image(o[idx].squeeze(), return_format='pil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = demo.pipeline.Z.weight\n",
    "print(latent[2376])\n",
    "print(latent[idcs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idcs = torch.randint(0,vals.dataset.N-2, size=(7,))\n",
    "print(idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals.dataloader.dataset[0]\n",
    "vals.dataloader.dataset.get_unsqueezed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clatent = latent[0:7]\n",
    "clatent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatches(idcs):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[DIK.IMAGE_IN].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = vals.getBatch(mode='sampleVal')\n",
    "\n",
    "batch[DIK.STYLE_LATENT]=clatent\n",
    "\n",
    "batch = vals.model.forward(batch, 0)\n",
    "img = batch[DIK.IMAGE_OUT]\n",
    "demo.visualizer_complete.tensor2image(img[2].squeeze(), return_format='pil')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python VariTex",
   "language": "python",
   "name": "varitex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
