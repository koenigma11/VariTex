{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of GLO as a replacement for the VariTex Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from PIL import Image  # for use of demos (in env if needed)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as fnn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "from torchvision.datasets import LSUN\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for GLO implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data\n",
    "2. Initialize Latent Space randomly or with PCA using a subset of the training data |Z| = (|data|, |latentCode|)\n",
    "3. Project to L2 ball\n",
    "    3. Don't forget to push tensors on Cuda and make z var\n",
    "4. For all images in training data\n",
    "    4. Generate Data\n",
    "    4. Calc loss and backprop\n",
    "    4. project Z to l2 ball\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Wraps another dataset to sample from. Returns the sampled indices during iteration.\n",
    "    In other words, instead of producing (X, y) it produces (X, y, idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base = base_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.base[idx]\n",
    "        return (img, label, idx)\n",
    "\n",
    "\n",
    "\"\"\"Project to L2 ball with radius 1\"\"\"\n",
    "\"\"\"Used after initialization as well as after any update.\"\"\"\n",
    "def projectToL2Ball(z):\n",
    "    ##project the vectors in z onto the l2 unit norm ball\n",
    "    return z / np.maximum(np.sqrt(np.sum(z**2, axis=1))[:, np.newaxis], 1)\n",
    "\n",
    "\"\"\"Gaussian kernel for Laplace loss\"\"\"\n",
    "def build_gauss_kernel(size=5, sigma=1.0, n_channels=1, cuda=False):\n",
    "    if size % 2 != 1:\n",
    "        raise ValueError(\"kernel size must be uneven\")\n",
    "    grid = np.float32(np.mgrid[0:size,0:size].T)\n",
    "    gaussian = lambda x: np.exp((x - size//2)**2/(-2*sigma**2))**2\n",
    "    kernel = np.sum(gaussian(grid), axis=2)\n",
    "    kernel /= np.sum(kernel)\n",
    "    # repeat same kernel across depth dimension\n",
    "    kernel = np.tile(kernel, (n_channels, 1, 1))\n",
    "    # conv weight should be (out_channels, groups/in_channels, h, w), \n",
    "    # and since we have depth-separable convolution we want the groups dimension to be 1\n",
    "    kernel = torch.FloatTensor(kernel[:, None, :, :])\n",
    "    if cuda:\n",
    "        kernel = kernel.cuda()\n",
    "    return Variable(kernel, requires_grad=False)\n",
    "\n",
    "\n",
    "def conv_gauss(img, kernel):\n",
    "    \"\"\" convolve img with a gaussian kernel that has been built with build_gauss_kernel \"\"\"\n",
    "    n_channels, _, kw, kh = kernel.shape\n",
    "    img = fnn.pad(img, (kw//2, kh//2, kw//2, kh//2), mode='replicate')\n",
    "    return fnn.conv2d(img, kernel, groups=n_channels)\n",
    "\n",
    "\n",
    "def laplacian_pyramid(img, kernel, max_levels=5):\n",
    "    current = img\n",
    "    pyr = []\n",
    "\n",
    "    for level in range(max_levels):\n",
    "        filtered = conv_gauss(current, kernel)\n",
    "        diff = current - filtered\n",
    "        pyr.append(diff)\n",
    "        current = fnn.avg_pool2d(filtered, 2)\n",
    "\n",
    "    pyr.append(current)\n",
    "    return pyr\n",
    "\n",
    "\n",
    "class LapLoss(nn.Module):\n",
    "    def __init__(self, max_levels=5, k_size=5, sigma=2.0):\n",
    "        super(LapLoss, self).__init__()\n",
    "        self.max_levels = max_levels\n",
    "        self.k_size = k_size\n",
    "        self.sigma = sigma\n",
    "        self._gauss_kernel = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if self._gauss_kernel is None or self._gauss_kernel.shape[1] != input.shape[1]:\n",
    "            self._gauss_kernel = build_gauss_kernel(\n",
    "                size=self.k_size, sigma=self.sigma, \n",
    "                n_channels=input.shape[1], cuda=input.is_cuda\n",
    "            )\n",
    "        pyr_input  = laplacian_pyramid( input, self._gauss_kernel, self.max_levels)\n",
    "        pyr_target = laplacian_pyramid(target, self._gauss_kernel, self.max_levels)\n",
    "        return sum(fnn.l1_loss(a, b) for a, b in zip(pyr_input, pyr_target))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varitex",
   "language": "python",
   "name": "varitex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
